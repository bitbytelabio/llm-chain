"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[236],{3905:(e,t,n)=>{n.d(t,{Zo:()=>s,kt:()=>h});var a=n(7294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function i(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},o=Object.keys(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)n=o[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var p=a.createContext({}),u=function(e){var t=a.useContext(p),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},s=function(e){var t=u(e.components);return a.createElement(p.Provider,{value:t},e.children)},c="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},m=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,o=e.originalType,p=e.parentName,s=i(e,["components","mdxType","originalType","parentName"]),c=u(n),m=l,h=c["".concat(p,".").concat(m)]||c[m]||d[m]||o;return n?a.createElement(h,r(r({ref:t},s),{},{components:n})):a.createElement(h,r({ref:t},s))}));function h(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var o=n.length,r=new Array(o);r[0]=m;var i={};for(var p in t)hasOwnProperty.call(t,p)&&(i[p]=t[p]);i.originalType=e,i[c]="string"==typeof e?e:l,r[1]=i;for(var u=2;u<o;u++)r[u]=n[u];return a.createElement.apply(null,r)}return a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},7413:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>p,contentTitle:()=>r,default:()=>d,frontMatter:()=>o,metadata:()=>i,toc:()=>u});var a=n(7462),l=(n(7294),n(3905));const o={},r="Tutorial: Getting Started using the LLAMA driver",i={unversionedId:"llama-tutorial",id:"llama-tutorial",title:"Tutorial: Getting Started using the LLAMA driver",description:"In this tutorial, you will learn how to set up an llm-project using the LLAMA drive. If you wish to use the other drivers you can skip this part of the tutorial.",source:"@site/docs/llama-tutorial.md",sourceDirName:".",slug:"/llama-tutorial",permalink:"/docs/llama-tutorial",draft:!1,editUrl:"https://github.com/sobelio/llm-chain/tree/main/docs/docs/llama-tutorial.md",tags:[],version:"current",frontMatter:{},sidebar:"sidebar",previous:{title:"Conversational Chains",permalink:"/docs/chains/conversational"}},p={},u=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Step 1: Create a new Rust project",id:"step-1-create-a-new-rust-project",level:2},{value:"Step 2: Add dependencies",id:"step-2-add-dependencies",level:2},{value:"Step 3: Update Rust",id:"step-3-update-rust",level:2},{value:"Step 4: Install and run llama.cpp",id:"step-4-install-and-run-llamacpp",level:2},{value:"Step 5: Add the llama.cpp driver",id:"step-5-add-the-llamacpp-driver",level:2},{value:"Step 6: Run the example code",id:"step-6-run-the-example-code",level:2}],s={toc:u},c="wrapper";function d(e){let{components:t,...n}=e;return(0,l.kt)(c,(0,a.Z)({},s,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h1",{id:"tutorial-getting-started-using-the-llama-driver"},"Tutorial: Getting Started using the LLAMA driver"),(0,l.kt)("p",null,"In this tutorial, you will learn how to set up an llm-project using the LLAMA drive. If you wish to use the other drivers you can skip this part of the tutorial."),(0,l.kt)("h2",{id:"prerequisites"},"Prerequisites"),(0,l.kt)("p",null,"To follow this tutorial, you will need:"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},"Ubuntu Linux 18.04 or higher"),(0,l.kt)("li",{parentName:"ul"},"Rust 1.71.0 or higher"),(0,l.kt)("li",{parentName:"ul"},"Cargo, the Rust package manager"),(0,l.kt)("li",{parentName:"ul"},"GCC/G++ 8 or higher"),(0,l.kt)("li",{parentName:"ul"},"A Hugging Face account1"),(0,l.kt)("li",{parentName:"ul"},"Git and Git LFS"),(0,l.kt)("li",{parentName:"ul"},"Pyenv, a Python version manager"),(0,l.kt)("li",{parentName:"ul"},"Python 3.11.3 or higher"),(0,l.kt)("li",{parentName:"ul"},"cmake, libclang-dev")),(0,l.kt)("p",null,"We tested using these exact software versions, but you should be able to get similar results using the latest versions. Also, there are many alternative ways to install and use these products. For example install Python via your Linux distribution's package manager. You can adapt this tutorial to your environment."),(0,l.kt)("h2",{id:"step-1-create-a-new-rust-project"},"Step 1: Create a new Rust project"),(0,l.kt)("p",null,"First, you will create a new Rust project using Cargo. To create a new project, open a terminal and run the following command:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"cargo new --bin llm-chain-demo\ncd llm-chain-demo\n")),(0,l.kt)("p",null,"This will create a new directory called ",(0,l.kt)("inlineCode",{parentName:"p"},"llm-chain-demo")," with the following structure:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"llm-chain-demo\n\u251c\u2500\u2500 Cargo.toml\n\u2514\u2500\u2500 src\n    \u2514\u2500\u2500 main.rs\n")),(0,l.kt)("p",null,"The ",(0,l.kt)("inlineCode",{parentName:"p"},"Cargo.toml")," file contains the metadata and dependencies of your project. The ",(0,l.kt)("inlineCode",{parentName:"p"},"src/main.rs")," file contains the main source code of your project."),(0,l.kt)("h2",{id:"step-2-add-dependencies"},"Step 2: Add dependencies"),(0,l.kt)("p",null,"To add these dependencies, run the following commands in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"cargo add llm-chain\ncargo add tokio --features all\n")),(0,l.kt)("h2",{id:"step-3-update-rust"},"Step 3: Update Rust"),(0,l.kt)("p",null,"The minimum version required to run this tutorial is Rust 1.65.0. At the time of this writing, the latest stable version is 1.71.0, so we'll use that. "),(0,l.kt)("p",null,"To switch to Rust 1.71.0, you need to use ",(0,l.kt)("inlineCode",{parentName:"p"},"rustup"),", which is a tool that helps you manage multiple versions of Rust on your system."),(0,l.kt)("p",null,"To install ",(0,l.kt)("inlineCode",{parentName:"p"},"rustup"),", follow the instruction on ",(0,l.kt)("a",{parentName:"p",href:"https://rustup.rs/"},"https://rustup.rs/")),(0,l.kt)("p",null,"To install Rust 1.71.0 using ",(0,l.kt)("inlineCode",{parentName:"p"},"rustup"),", run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"rustup install 1.71.0\n")),(0,l.kt)("p",null,"To switch to Rust 1.71.0 as the default version for your project, run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"rustup default 1.71.0\n")),(0,l.kt)("p",null,"You can verify that you are using Rust 1.71.0 by running the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"rustc --version\n")),(0,l.kt)("p",null,"This should output something like this:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"rustc 1.71.0 (8ede3aae2 2023-07-12)\n")),(0,l.kt)("h2",{id:"step-4-install-and-run-llamacpp"},"Step 4: Install and run llama.cpp"),(0,l.kt)("p",null,"Now that you have set up your Rust project and switched to the correct version of Rust, you need to install and run llama.cpp, which is a C++ implementation of LLaMa models for inference. "),(0,l.kt)("p",null,"llama.cpp requires GCC and G++ 8 or newer, if your distribution use GCC/G++ 7 by default, use these commands to update:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"sudo apt install gcc-8 g++-8\nsudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-8 10\nsudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-8 10\nsudo update-alternatives --install /usr/bin/cc cc /usr/bin/gcc 30\nsudo update-alternatives --set cc /usr/bin/gcc\n\nsudo update-alternatives --install /usr/bin/c++ c++ /usr/bin/g++ 30\nsudo update-alternatives --set c++ /usr/bin/g++\n")),(0,l.kt)("p",null,"To install llama.cpp, you need to clone its repository from GitHub and build it from source. To do that, run the following commands in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"git clone https://github.com/ggerganov/llama.cpp\ncd llama.cpp\nmake\n")),(0,l.kt)("p",null,"Notice that we clone the ",(0,l.kt)("inlineCode",{parentName:"p"},"llama.cpp")," folder inside the ",(0,l.kt)("inlineCode",{parentName:"p"},"llm-chain-demo")," folder. "),(0,l.kt)("p",null,"To run llama.cpp, you need to download a LLaMa model and convert it to a binary format that llama.cpp can read. In this tutorial, you will use the Alpaca model."),(0,l.kt)("p",null,"To download the Alpaca model, you need to have a Hugging Face account and install Git LFS. Hugging Face is a platform that hosts and distributes various natural language processing models, including LLaMa models. Git LFS is an extension for Git that allows you to store large files on GitHub. Because these LLMs are usually quite big, Hugging Face use Git LFS to allow you to download them using git."),(0,l.kt)("p",null,"To create a Hugging Face account, go to ",(0,l.kt)("a",{parentName:"p",href:"https://huggingface.co/"},"Hugging Face")," and sign up."),(0,l.kt)("p",null,"To install Git LFS, run the following commands in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bash\nsudo apt install git-lfs\ngit lfs install\n")),(0,l.kt)("p",null,"This will download and install Git LFS on your system."),(0,l.kt)("p",null,"To download the Alpaca model, run the following commands in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"cd ./models\ngit clone https://huggingface.co/chavinlo/alpaca-native\n")),(0,l.kt)("p",null,"This will clone the Alpaca model repository to your ",(0,l.kt)("inlineCode",{parentName:"p"},"models")," directory."),(0,l.kt)("p",null,"To convert the Alpaca model to the format llama.cpp accepts, you need to install Python and run a conversion script. In this tutorial, you will use Python 3.11.3, which is the latest stable version of Python at the time of writing this tutorial."),(0,l.kt)("p",null,"To install Python 3.11.3, you need to use ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv"),", which is a tool that helps you manage multiple versions of Python on your system."),(0,l.kt)("p",null,"To install ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv"),", run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"curl https://pyenv.run | bash\n")),(0,l.kt)("p",null,"Then, you need to install the ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv-virtualenv")," plugin to let ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv")," manage virtualenv for you. Run this command to install ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv-virtualenv"),":"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"git clone https://github.com/pyenv/pyenv-virtualenv.git $(pyenv root)/plugins/pyenv-virtualenv\n")),(0,l.kt)("p",null,"If you use zsh, add the following lines to your ",(0,l.kt)("inlineCode",{parentName:"p"},"~/.zshrc")," file, run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"echo 'eval \"$(pyenv init -)\"' >> ~/.zshrc\necho 'eval \"$(pyenv virtualenv-init -)\"' >> ~/.zshrc\nsource ~/.zshrc\n")),(0,l.kt)("p",null,"Or replace it with ",(0,l.kt)("inlineCode",{parentName:"p"},"~/.bashrc")," if you prefer bash."),(0,l.kt)("p",null,"This will enable ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv")," to manage your Python versions and virtual environments."),(0,l.kt)("p",null,"To install Python 3.11.3 using ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv"),", run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"pyenv install 3.11.3\n")),(0,l.kt)("p",null,"To create a virtual environment for Python 3.11.3 using ",(0,l.kt)("inlineCode",{parentName:"p"},"pyenv"),", run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"pyenv virtualenv 3.11.3 llama\n")),(0,l.kt)("p",null,"To activate the virtual environment, run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"pyenv activate llama\n")),(0,l.kt)("p",null,"This will activate the virtual environment and change your prompt to indicate that you are using it."),(0,l.kt)("p",null,"To install the required Python packages for the conversion script, run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"# in the llama.cpp root directory\npip install -r requirements.txt\n")),(0,l.kt)("p",null,"With the Python dependencies installed, you need to run the conversion script that will convert the Alpaca model to a binary format that llama.cpp can read. To do that, run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"python convert.py /models/alpaca-native\n")),(0,l.kt)("p",null,"This will run the ",(0,l.kt)("inlineCode",{parentName:"p"},"convert.py")," script that is located in the ",(0,l.kt)("inlineCode",{parentName:"p"},"llama.cpp")," directory. The script will take the Alpaca model directory as an argument and output a binary file called ",(0,l.kt)("inlineCode",{parentName:"p"},"ggml-model-f32.bin")," in the same directory. "),(0,l.kt)("p",null,"To test llama.cpp with the Alpaca model, run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'./main -m models/alpaca-native/ggml-model-f32.bin -n 128 -p "I love Rust because"\n')),(0,l.kt)("p",null,"This will run the ",(0,l.kt)("inlineCode",{parentName:"p"},"main")," executable that is located in the ",(0,l.kt)("inlineCode",{parentName:"p"},"llama.cpp")," directory. The executable will take two arguments: ",(0,l.kt)("inlineCode",{parentName:"p"},"-m"),", ",(0,l.kt)("inlineCode",{parentName:"p"},"-n")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"-p")," The ",(0,l.kt)("inlineCode",{parentName:"p"},"-m")," argument specifies the path to the model binary file. The ",(0,l.kt)("inlineCode",{parentName:"p"},"-n")," argument specifies the number of tokens to generate for each input. The ",(0,l.kt)("inlineCode",{parentName:"p"},"-p")," argument specifies the prompt you send to the LLM."),(0,l.kt)("p",null,"The model may take some time to load. You should see something like this in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"main: build = 849 (d01bccd)\nmain: seed  = 1690205678\nllama.cpp: loading model from models/alpaca-native/ggml-model-f32.bin\nllama_model_load_internal: format     = ggjt v1 (pre #1405)\n...\nllama_new_context_with_model: kv self size  =  256.00 MB\n\nsystem_info: n_threads = 4 / 8 | AVX = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \nsampling: repeat_last_n = 64, repeat_penalty = 1.100000, presence_penalty = 0.000000, frequency_penalty = 0.000000, top_k = 40, tfs_z = 1.000000, top_p = 0.950000, typical_p = 1.000000, temp = 0.800000, mirostat = 0, mirostat_lr = 0.100000, mirostat_ent = 5.000000\ngenerate: n_ctx = 512, n_batch = 512, n_predict = 128, n_keep = 0\n\n I love Rust because it allows me to create things that are both practical and beautiful. I can design objects that are functional, reliable, and secure - all while still looking great. It\u2019s also a fun language to work with, as it encourages creativity through its focus on code spelunking and efficient algorithmic thinking. [end of text]\n\nllama_print_timings:        load time = 113290.18 ms\n...\nllama_print_timings:       total time = 44284.74 ms\n")),(0,l.kt)("p",null,"This means that llama.cpp has successfully loaded the Alpaca model and generated text based on your prompt."),(0,l.kt)("h2",{id:"step-5-add-the-llamacpp-driver"},"Step 5: Add the llama.cpp driver"),(0,l.kt)("p",null,"Now we can get back to the ",(0,l.kt)("inlineCode",{parentName:"p"},"llm-chain-demo")," Rust project. To use the LLAMA driver, you need to add it as a dependency to your Rust project. You can run the following command in the terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"cargo add llm-chain-llama\n")),(0,l.kt)("h2",{id:"step-6-run-the-example-code"},"Step 6: Run the example code"),(0,l.kt)("p",null,"To run the example code, you need to copy and paste it into your ",(0,l.kt)("inlineCode",{parentName:"p"},"src/main.rs")," file. The example code creates a LLAMA executor with the Alpaca model and generates text for a given prompt."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-rust"},'use llm_chain::executor;\nuse llm_chain::{parameters, prompt};\nuse llm_chain::options::*;\nuse llm_chain::options;\n\n#[tokio::main(flavor = "current_thread")]\nasync fn main() -> Result<(), Box<dyn std::error::Error>> {\n    let opts = options!(\n        Model: ModelRef::from_path("./llama.cpp/models/alpaca-native/ggml-model-f32.bin"), // Notice that we reference the model binary path\n        ModelType: "llama",\n        MaxContextSize: 512_usize,\n        NThreads: 4_usize,\n        MaxTokens: 0_usize,\n        TopK: 40_i32,\n        TopP: 0.95,\n        TfsZ: 1.0,\n        TypicalP: 1.0,\n        Temperature: 0.8,\n        RepeatPenalty: 1.1,\n        RepeatPenaltyLastN: 64_usize,\n        FrequencyPenalty: 0.0,\n        PresencePenalty: 0.0,\n        Mirostat: 0_i32,\n        MirostatTau: 5.0,\n        MirostatEta: 0.1,\n        PenalizeNl: true,\n        StopSequence: vec!["\\n".to_string()]\n    );\n    let exec = executor!(llama, opts)?;\n    let res = prompt!("I love Rust because")\n        .run(\n            &parameters!(),\n            &exec,\n        )\n        .await?;\n    println!("{}", res.to_immediate().await?);\n    Ok(())\n}\n')),(0,l.kt)("p",null,"When we set up llama.cpp, we use ",(0,l.kt)("inlineCode",{parentName:"p"},"make")," to compile it. But llama.cpp can also be compiled with ",(0,l.kt)("inlineCode",{parentName:"p"},"cmake"),". the ",(0,l.kt)("inlineCode",{parentName:"p"},"llm-chain-llama")," crate uses ",(0,l.kt)("inlineCode",{parentName:"p"},"llm-chain-llama-sys")," internally, and ",(0,l.kt)("inlineCode",{parentName:"p"},"llm-chain-llama.sys")," uses ",(0,l.kt)("inlineCode",{parentName:"p"},"cmake")," to compile the bindings for llama.cpp. Before you run the example code, you may need to install some additional packages for the compilation, such as ",(0,l.kt)("inlineCode",{parentName:"p"},"libclang-dev")," and ",(0,l.kt)("inlineCode",{parentName:"p"},"cmake"),". To install ",(0,l.kt)("inlineCode",{parentName:"p"},"libclang-dev"),", run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"sudo apt install libclang-dev\n")),(0,l.kt)("p",null,"This will install the Clang library development files on your system."),(0,l.kt)("p",null,"To install ",(0,l.kt)("inlineCode",{parentName:"p"},"cmake"),", you can to use a PPA from Kitware, which provides the latest version of ",(0,l.kt)("inlineCode",{parentName:"p"},"cmake"),". You can also compile from source if you have concerns using 3rd-party PPA. To do that, run the following commands in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},'wget -O - https://apt.kitware.com/keys/kitware-archive-latest.asc 2>/dev/null | gpg --dearmor - | sudo tee /etc/apt/trusted.gpg.d/kitware.gpg >/dev/null\nsudo apt-add-repository "deb https://apt.kitware.com/ubuntu/ $(lsb_release -cs) main"\nsudo apt update\nsudo apt install kitware-archive-keyring\nsudo apt update\nsudo apt install cmake\n')),(0,l.kt)("p",null,"To run your program, run the following command in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"cargo run\n")),(0,l.kt)("p",null,"You should see something like this in your terminal:"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"   Compiling llm-chain-demo v0.1.0 (/home/ubuntu/environment/llm-chain-demo)\n    Finished dev [unoptimized + debuginfo] target(s) in 9.05s\n     Running `target/debug/llm-chain-demo`\n...\nI love Rust because it allows me to create things that are both practical and beautiful. I can design objects that are functional, reliable, and secure - all while still looking great. It\u2019s also a fun language to work with, as it encourages creativity through its focus on code spelunking and efficient algorithmic thinking. [end of text]\n...\n")),(0,l.kt)("p",null,"This is the text generated through the llm-chain and the LLAMA driver based on your prompt."),(0,l.kt)("p",null,"Congratulations! You have successfully run the example code using the llama.cpp driver. You can experiment with different models, model parameters and prompts."))}d.isMDXComponent=!0}}]);